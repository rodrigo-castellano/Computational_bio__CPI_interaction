{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNyCmJ8BQTwDrafuVX+Vw/U"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive') \n","# %cd  /content/drive/MyDrive/Colab_Notebooks/comp_bio"],"metadata":{"id":"QcKhxDhXdjXc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install import-ipynb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"13mS7re1bPIX","executionInfo":{"status":"ok","timestamp":1648470070967,"user_tz":-120,"elapsed":11002,"user":{"displayName":"Rodrigo Castellano","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09621897243893675794"}},"outputId":"ff412ba9-f602-4b9f-8340-eb84d1f77d31"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting import-ipynb\n","  Downloading import-ipynb-0.1.3.tar.gz (4.0 kB)\n","Building wheels for collected packages: import-ipynb\n","  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-py3-none-any.whl size=2975 sha256=b4f2633fa7bebdfa8c8e6dfd9cedb41d237c858a9e561ee3e19cc1c2c71a5dcd\n","  Stored in directory: /root/.cache/pip/wheels/b1/5e/dc/79780689896a056199b0b9f24471e3ee184fbd816df355d5f0\n","Successfully built import-ipynb\n","Installing collected packages: import-ipynb\n","Successfully installed import-ipynb-0.1.3\n"]}]},{"cell_type":"code","source":["import import_ipynb\n","import Radam\n","import lookahead\n","from lookahead import Lookahead\n","from Radam import AdamW, PlainRAdam, RAdam"],"metadata":{"id":"ROc5TjkdbQk4","executionInfo":{"status":"error","timestamp":1649003686366,"user_tz":-120,"elapsed":245,"user":{"displayName":"Rodrigo Castellano","userId":"09621897243893675794"}},"colab":{"base_uri":"https://localhost:8080/","height":391},"outputId":"52810baf-0ea3-4c82-b1f9-daa294556fe0"},"execution_count":1,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-8a646d76e0d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mimport_ipynb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRadam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlookahead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlookahead\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLookahead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mRadam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPlainRAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'import_ipynb'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import math\n","import numpy as np\n","from sklearn.metrics import roc_auc_score, precision_score, recall_score,precision_recall_curve, auc\n","# from Radam import *\n","# from lookahead import Lookahead"],"metadata":{"id":"mdTfupqtbKcu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SelfAttention(nn.Module):\n","    def __init__(self, hid_dim, n_heads, dropout, device):\n","        super().__init__()\n","\n","        self.hid_dim = hid_dim\n","        self.n_heads = n_heads\n","\n","        assert hid_dim % n_heads == 0\n","\n","        self.w_q = nn.Linear(hid_dim, hid_dim)\n","        self.w_k = nn.Linear(hid_dim, hid_dim)\n","        self.w_v = nn.Linear(hid_dim, hid_dim)\n","\n","        self.fc = nn.Linear(hid_dim, hid_dim)\n","\n","        self.do = nn.Dropout(dropout)\n","\n","        self.scale = torch.sqrt(torch.FloatTensor([hid_dim // n_heads])).to(device)\n","\n","    def forward(self, query, key, value, mask=None):\n","        '''Summary: takes Query, Key, Values matrices and copmute score. For that, compute the entropy(or Energy). \n","        Then, apply softmax and compute the product of that with the Value matrix. It does a lot of reshapes'''\n","        bsz = query.shape[0]\n","\n","        # query = key = value [batch size, sent len, hid dim]\n","\n","        Q = self.w_q(query)\n","        K = self.w_k(key)\n","        V = self.w_v(value)\n","\n","        # Q, K, V = [batch size, sent len, hid dim]\n","\n","        Q = Q.view(bsz, -1, self.n_heads, self.hid_dim // self.n_heads).permute(0, 2, 1, 3)\n","        K = K.view(bsz, -1, self.n_heads, self.hid_dim // self.n_heads).permute(0, 2, 1, 3)\n","        V = V.view(bsz, -1, self.n_heads, self.hid_dim // self.n_heads).permute(0, 2, 1, 3)\n","\n","        # K, V = [batch size, n heads, sent len_K, hid dim // n heads]\n","        # Q = [batch size, n heads, sent len_q, hid dim // n heads]\n","        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n","\n","        # energy = [batch size, n heads, sent len_Q, sent len_K]\n","        if mask is not None:\n","            energy = energy.masked_fill(mask == 0, -1e10)\n","\n","        attention = self.do(F.softmax(energy, dim=-1))\n","\n","        # attention = [batch size, n heads, sent len_Q, sent len_K]\n","\n","        x = torch.matmul(attention, V)\n","\n","        # x = [batch size, n heads, sent len_Q, hid dim // n heads]\n","\n","        x = x.permute(0, 2, 1, 3).contiguous()\n","\n","        # x = [batch size, sent len_Q, n heads, hid dim // n heads]\n","\n","        x = x.view(bsz, -1, self.n_heads * (self.hid_dim // self.n_heads))\n","\n","        # x = [batch size, src sent len_Q, hid dim]\n","\n","        x = self.fc(x)\n","\n","        # x = [batch size, sent len_Q, hid dim]\n","\n","        return x"],"metadata":{"id":"aF_jmHDUe_hE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Encoder(nn.Module):\n","    \"\"\"protein feature extraction.\"\"\"\n","    def __init__(self, protein_dim, hid_dim, n_layers,kernel_size , dropout, device):\n","        super().__init__()\n","\n","        assert kernel_size % 2 == 1, \"Kernel size must be odd (for now)\"\n","\n","        self.input_dim = protein_dim\n","        self.hid_dim = hid_dim\n","        self.kernel_size = kernel_size\n","        self.dropout = dropout\n","        self.n_layers = n_layers\n","        self.device = device\n","        #self.pos_embedding = nn.Embedding(1000, hid_dim)\n","        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n","        self.convs = nn.ModuleList([nn.Conv1d(hid_dim, 2*hid_dim, kernel_size, padding=(kernel_size-1)//2) for _ in range(self.n_layers)])   # convolutional layers\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(self.input_dim, self.hid_dim)\n","        self.gn = nn.GroupNorm(8, hid_dim * 2)\n","        self.ln = nn.LayerNorm(hid_dim)\n","\n","    def forward(self, protein):\n","        '''Takes the protein and applies a feed forward, and a loop of: dropout, convolutional, GLU activation function,\n","        residual connection and layer normalization'''\n","        #pos = torch.arange(0, protein.shape[1]).unsqueeze(0).repeat(protein.shape[0], 1).to(self.device)\n","        #protein = protein + self.pos_embedding(pos)\n","        #protein = [batch size, protein len,protein_dim]\n","        conv_input = self.fc(protein)\n","        # conv_input=[batch size,protein len,hid dim]\n","        #permute for convolutional layer\n","        conv_input = conv_input.permute(0, 2, 1)\n","        #conv_input = [batch size, hid dim, protein len]\n","        for i, conv in enumerate(self.convs):\n","            #pass through convolutional layer\n","            conved = conv(self.dropout(conv_input))\n","            #conved = [batch size, 2*hid dim, protein len]\n","\n","            #pass through GLU activation function\n","            conved = F.glu(conved, dim=1)\n","            #conved = [batch size, hid dim, protein len]\n","\n","            #apply residual connection / high way\n","            conved = (conved + conv_input) * self.scale\n","            #conved = [batch size, hid dim, protein len]\n","\n","            #set conv_input to conved for next loop iteration\n","            conv_input = conved\n","\n","        conved = conved.permute(0, 2, 1)\n","        # conved = [batch size,protein len,hid dim]\n","        conved = self.ln(conved)\n","        return conved\n"],"metadata":{"id":"L268VkMZfDc2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PositionwiseFeedforward(nn.Module):\n","    '''https://ai.stackexchange.com/questions/15524/why-would-you-implement-the-position-wise-feed-forward-network-of-the-transforme \n","    In this class an input w/ (input,hidden) goes through fc layer to (input,pf_dim) and goes back to (input,hidden)\n","    This is done to approximate well a function thanks to using a veery wide fc layer'''\n","    def __init__(self, hid_dim, pf_dim, dropout):\n","        super().__init__()\n","\n","        self.hid_dim = hid_dim\n","        self.pf_dim = pf_dim\n","\n","        self.fc_1 = nn.Conv1d(hid_dim, pf_dim, 1)  # convolution neural units\n","        self.fc_2 = nn.Conv1d(pf_dim, hid_dim, 1)  # convolution neural units\n","\n","        self.do = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        # x = [batch size, sent len, hid dim]\n","\n","        x = x.permute(0, 2, 1)\n","\n","        # x = [batch size, hid dim, sent len]\n","\n","        x = self.do(F.relu(self.fc_1(x)))\n","\n","        # x = [batch size, pf dim, sent len]\n","\n","        x = self.fc_2(x)\n","\n","        # x = [batch size, hid dim, sent len]\n","\n","        x = x.permute(0, 2, 1)\n","\n","        # x = [batch size, sent len, hid dim]\n","\n","        return x"],"metadata":{"id":"VX1HGlO0fGa5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DecoderLayer(nn.Module):\n","    def __init__(self, hid_dim, n_heads, pf_dim, self_attention, positionwise_feedforward, dropout, device):\n","        super().__init__()\n","\n","        self.ln = nn.LayerNorm(hid_dim)\n","        self.sa = self_attention(hid_dim, n_heads, dropout, device)\n","        self.ea = self_attention(hid_dim, n_heads, dropout, device)\n","        self.pf = positionwise_feedforward(hid_dim, pf_dim, dropout)\n","        self.do = nn.Dropout(dropout)\n","\n","    def forward(self, trg, src, trg_mask=None, src_mask=None):\n","        # trg = [batch_size, compound len, atom_dim]\n","        # src = [batch_size, protein len, hid_dim] # encoder output\n","        # trg_mask = [batch size, compound sent len]\n","        # src_mask = [batch size, protein len]\n","\n","        trg = self.ln(trg + self.do(self.sa(trg, trg, trg, trg_mask)))\n","\n","        trg = self.ln(trg + self.do(self.ea(trg, src, src, src_mask)))\n","\n","        trg = self.ln(trg + self.do(self.pf(trg)))\n","\n","        return trg\n","\n","\n","class Decoder(nn.Module):\n","    \"\"\" compound feature extraction.\"\"\"\n","    def __init__(self, atom_dim, hid_dim, n_layers, n_heads, pf_dim, decoder_layer, self_attention,\n","                 positionwise_feedforward, dropout, device):\n","        super().__init__()\n","        self.ln = nn.LayerNorm(hid_dim)\n","        self.output_dim = atom_dim\n","        self.hid_dim = hid_dim\n","        self.n_layers = n_layers\n","        self.n_heads = n_heads\n","        self.pf_dim = pf_dim\n","        self.decoder_layer = decoder_layer\n","        self.self_attention = self_attention\n","        self.positionwise_feedforward = positionwise_feedforward\n","        self.dropout = dropout\n","        self.device = device\n","        self.sa = self_attention(hid_dim, n_heads, dropout, device)\n","        self.layers = nn.ModuleList(\n","            [decoder_layer(hid_dim, n_heads, pf_dim, self_attention, positionwise_feedforward, dropout, device)\n","             for _ in range(n_layers)])\n","        self.ft = nn.Linear(atom_dim, hid_dim)\n","        self.do = nn.Dropout(dropout)\n","        self.fc_1 = nn.Linear(hid_dim, 256)\n","        self.fc_2 = nn.Linear(256, 2)\n","        self.gn = nn.GroupNorm(8, 256)\n","\n","    def forward(self, trg, src, trg_mask=None,src_mask=None):\n","        # trg = [batch_size, compound len, atom_dim]\n","        # src = [batch_size, protein len, hid_dim] # encoder output\n","        trg = self.ft(trg)\n","\n","        # trg = [batch size, compound len, hid dim]\n","\n","        for layer in self.layers:\n","            trg = layer(trg, src,trg_mask,src_mask)\n","\n","        # trg = [batch size, compound len, hid dim]\n","        \"\"\"Use norm to determine which atom is significant. \"\"\"\n","        norm = torch.norm(trg, dim=2)\n","        # norm = [batch size,compound len]\n","        norm = F.softmax(norm, dim=1)\n","        # norm = [batch size,compound len]\n","        # trg = torch.squeeze(trg,dim=0)\n","        # norm = torch.squeeze(norm,dim=0)\n","        sum = torch.zeros((trg.shape[0], self.hid_dim)).to(self.device)\n","        for i in range(norm.shape[0]):\n","            for j in range(norm.shape[1]):\n","                v = trg[i, j, ]\n","                v = v * norm[i, j]\n","                sum[i, ] += v\n","        # sum = [batch size,hid_dim]\n","        label = F.relu(self.fc_1(sum))\n","        label = self.fc_2(label)\n","        return label\n","\n"],"metadata":{"id":"z_MN_KC-fNkQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Predictor(nn.Module):\n","    def __init__(self, encoder, decoder, device, atom_dim=34):\n","        super().__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","        self.weight = nn.Parameter(torch.FloatTensor(atom_dim, atom_dim))\n","        self.init_weight()\n","\n","    def init_weight(self):\n","        stdv = 1. / math.sqrt(self.weight.size(1))\n","        self.weight.data.uniform_(-stdv, stdv)\n","\n","    def gcn(self, input, adj):\n","        # input =[batch,num_node, atom_dim]\n","        # adj = [batch,num_node, num_node]\n","        support = torch.matmul(input, self.weight)\n","        # support =[batch,num_node,atom_dim]\n","        output = torch.bmm(adj, support)\n","        # output = [batch,num_node,atom_dim]\n","        return output\n","\n","    def make_masks(self, atom_num, protein_num, compound_max_len, protein_max_len):\n","        N = len(atom_num)  # batch size\n","        compound_mask = torch.zeros((N, compound_max_len))\n","        protein_mask = torch.zeros((N, protein_max_len))\n","        for i in range(N):\n","            compound_mask[i, :atom_num[i]] = 1\n","            protein_mask[i, :protein_num[i]] = 1\n","        compound_mask = compound_mask.unsqueeze(1).unsqueeze(3).to(self.device)\n","        protein_mask = protein_mask.unsqueeze(1).unsqueeze(2).to(self.device)\n","        return compound_mask, protein_mask\n","\n","\n","    def forward(self, compound, adj,  protein,atom_num,protein_num):\n","        # compound = [batch,atom_num, atom_dim]\n","        # adj = [batch,atom_num, atom_num]\n","        # protein = [batch,protein len, 100]\n","        compound_max_len = compound.shape[1]\n","        protein_max_len = protein.shape[1]\n","        compound_mask, protein_mask = self.make_masks(atom_num, protein_num, compound_max_len, protein_max_len)\n","        compound = self.gcn(compound, adj)\n","        # compound = torch.unsqueeze(compound, dim=0)\n","        # compound = [batch size=1 ,atom_num, atom_dim]\n","\n","        # protein = torch.unsqueeze(protein, dim=0)\n","        # protein =[ batch size=1,protein len, protein_dim]\n","        enc_src = self.encoder(protein)\n","        # enc_src = [batch size, protein len, hid dim]\n","\n","        out = self.decoder(compound, enc_src, compound_mask, protein_mask)\n","        # out = [batch size, 2]\n","        # out = torch.squeeze(out, dim=0)\n","        return out\n","\n","    def __call__(self, data, train=True):\n","\n","        compound, adj, protein, correct_interaction ,atom_num,protein_num = data\n","        # compound = compound.to(self.device)\n","        # adj = adj.to(self.device)\n","        # protein = protein.to(self.device)\n","        # correct_interaction = correct_interaction.to(self.device)\n","        Loss = nn.CrossEntropyLoss()\n","\n","        if train:\n","            predicted_interaction = self.forward(compound, adj, protein,atom_num,protein_num)\n","            print('predicted_interaction',predicted_interaction.shape)\n","            loss = Loss(predicted_interaction, correct_interaction)\n","            return loss\n","\n","        else:\n","            #compound = compound.unsqueeze(0)\n","            #adj = adj.unsqueeze(0)\n","            #protein = protein.unsqueeze(0)\n","            #correct_interaction = correct_interaction.unsqueeze(0)\n","            predicted_interaction = self.forward(compound, adj, protein,atom_num,protein_num)\n","            correct_labels = correct_interaction.to('cpu').data.numpy()\n","            ys = F.softmax(predicted_interaction, 1).to('cpu').data.numpy()\n","            predicted_labels = np.argmax(ys, axis=1)\n","            predicted_scores = ys[:, 1]\n","            return correct_labels, predicted_labels, predicted_scores"],"metadata":{"id":"jsLs_hKafTwZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZm8Ejlva_wm"},"outputs":[],"source":["def pack(atoms, adjs, proteins, labels, device):\n","    atoms_len = 0\n","    proteins_len = 0\n","    N = len(atoms)\n","    atom_num = []\n","    for atom in atoms:\n","        atom_num.append(atom.shape[0])\n","        if atom.shape[0] >= atoms_len:\n","            atoms_len = atom.shape[0]\n","    protein_num = []\n","    for protein in proteins:\n","        protein_num.append(protein.shape[0])\n","        if protein.shape[0] >= proteins_len:\n","            proteins_len = protein.shape[0]\n","    atoms_new = torch.zeros((N,atoms_len,34), device=device)\n","    i = 0\n","    for atom in atoms:\n","        a_len = atom.shape[0]\n","        atoms_new[i, :a_len, :] = atom\n","        i += 1\n","    adjs_new = torch.zeros((N, atoms_len, atoms_len), device=device)\n","    i = 0\n","    for adj in adjs:\n","        a_len = adj.shape[0]\n","        adj = adj + torch.eye(a_len, device=device)\n","        adjs_new[i, :a_len, :a_len] = adj\n","        i += 1\n","    proteins_new = torch.zeros((N, proteins_len, 100), device=device)\n","    i = 0\n","    for protein in proteins:\n","        a_len = protein.shape[0]\n","        proteins_new[i, :a_len, :] = protein\n","        i += 1\n","    labels_new = torch.zeros(N, dtype=torch.long, device=device)\n","    i = 0\n","    for label in labels:\n","        labels_new[i] = label\n","        i += 1\n","    return (atoms_new, adjs_new, proteins_new, labels_new, atom_num, protein_num)\n","\n","\n","class Trainer(object):\n","    def __init__(self, model, lr, weight_decay, batch):\n","        self.model = model\n","        # w - L2 regularization ; b - not L2 regularization\n","        weight_p, bias_p = [], []\n","\n","        for p in self.model.parameters():\n","            if p.dim() > 1:\n","                nn.init.xavier_uniform_(p)\n","\n","        for name, p in self.model.named_parameters():\n","            if 'bias' in name:\n","                bias_p += [p]\n","            else:\n","                weight_p += [p]\n","        # self.optimizer = optim.Adam([{'params': weight_p, 'weight_decay': weight_decay}, {'params': bias_p, 'weight_decay': 0}], lr=lr)\n","        self.optimizer_inner = RAdam(\n","            [{'params': weight_p, 'weight_decay': weight_decay}, {'params': bias_p, 'weight_decay': 0}], lr=lr)\n","        self.optimizer = Lookahead(self.optimizer_inner, k=5, alpha=0.5)\n","        self.batch = batch\n","        print('vaaaaaaaaaaaaaaaaaaaaaaaaamos')\n","\n","    def train(self, dataset, device):\n","        print('vaaaaaaaaaaaaaaaaaaaaaaaaamo')\n","        self.model.train()\n","        np.random.shuffle(dataset)\n","        N = len(dataset)\n","        loss_total = 0\n","        i = 0\n","        self.optimizer.zero_grad()\n","        adjs, atoms, proteins, labels = [], [], [], []\n","        for data in dataset:\n","            i = i+1\n","            atom, adj, protein, label = data\n","            adjs.append(adj)\n","            atoms.append(atom)\n","            proteins.append(protein)\n","            labels.append(label)\n","            if i % 8 == 0 or i == N:\n","                data_pack = pack(atoms, adjs, proteins, labels, device)\n","                loss = self.model(data_pack)\n","                # loss = loss / self.batch\n","                loss.backward()\n","                # torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10)\n","                adjs, atoms, proteins, labels = [], [], [], []\n","            else:\n","                continue\n","            if i % self.batch == 0 or i == N:\n","                self.optimizer.step()\n","                self.optimizer.zero_grad()\n","            loss_total += loss.item()\n","        return loss_total\n","\n","\n","class Tester(object):\n","    def __init__(self, model):\n","        self.model = model\n","\n","    def test(self, dataset):\n","        self.model.eval()\n","        N = len(dataset)\n","        T, Y, S = [], [], []\n","        with torch.no_grad():\n","            for data in dataset:\n","                adjs, atoms, proteins, labels = [], [], [], []\n","                atom, adj, protein, label = data\n","                adjs.append(adj)\n","                atoms.append(atom)\n","                proteins.append(protein)\n","                labels.append(label)\n","                data = pack(atoms,adjs,proteins, labels, self.model.device)\n","                correct_labels, predicted_labels, predicted_scores = self.model(data, train=False)\n","                T.extend(correct_labels)\n","                Y.extend(predicted_labels)\n","                S.extend(predicted_scores)\n","        AUC = roc_auc_score(T, S)\n","        tpr, fpr, _ = precision_recall_curve(T, S)\n","        PRC = auc(fpr, tpr)\n","        return AUC, PRC\n","\n","    def save_AUCs(self, AUCs, filename):\n","        with open(filename, 'a') as f:\n","            f.write('\\t'.join(map(str, AUCs)) + '\\n')\n","\n","    def save_model(self, model, filename):\n","        torch.save(model.state_dict(), filename)\n"]},{"cell_type":"code","source":["  "],"metadata":{"id":"gp-1zaCcbjeL"},"execution_count":null,"outputs":[]}]}