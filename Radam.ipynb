{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Radam.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyObEi4BO51V9ygrV1DeGqbT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import math\n","import torch\n","from torch.optim.optimizer import Optimizer #, required"],"metadata":{"id":"20P8rR9zdHYg","executionInfo":{"status":"ok","timestamp":1648302010022,"user_tz":-60,"elapsed":7478,"user":{"displayName":"Rodrigo Castellano","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09621897243893675794"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["class RAdam(Optimizer):\n","\n","    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n","        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n","        self.buffer = [[None, None, None] for ind in range(10)]\n","        super(RAdam, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(RAdam, self).__setstate__(state)\n","\n","    def step(self, closure=None):\n","\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","                grad = p.grad.data.float()\n","                if grad.is_sparse:\n","                    raise RuntimeError('RAdam does not support sparse gradients')\n","\n","                p_data_fp32 = p.data.float()\n","\n","                state = self.state[p]\n","\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n","                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n","                else:\n","                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n","                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n","\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","                beta1, beta2 = group['betas']\n","\n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","\n","                state['step'] += 1\n","                buffered = self.buffer[int(state['step'] % 10)]\n","                if state['step'] == buffered[0]:\n","                    N_sma, step_size = buffered[1], buffered[2]\n","                else:\n","                    buffered[0] = state['step']\n","                    beta2_t = beta2 ** state['step']\n","                    N_sma_max = 2 / (1 - beta2) - 1\n","                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n","                    buffered[1] = N_sma\n","\n","                    # more conservative since it's an approximated value\n","                    if N_sma >= 5:\n","                        step_size = group['lr'] * math.sqrt(\n","                            (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (\n","                                        N_sma_max - 2)) / (1 - beta1 ** state['step'])\n","                    else:\n","                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n","                    buffered[2] = step_size\n","\n","                if group['weight_decay'] != 0:\n","                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n","\n","                # more conservative since it's an approximated value\n","                if N_sma >= 5:\n","                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n","                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n","                else:\n","                    p_data_fp32.add_(-step_size, exp_avg)\n","\n","                p.data.copy_(p_data_fp32)\n","\n","        return loss\n","\n","\n","class PlainRAdam(Optimizer):\n","\n","    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n","        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n","\n","        super(PlainRAdam, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(PlainRAdam, self).__setstate__(state)\n","\n","    def step(self, closure=None):\n","\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","                grad = p.grad.data.float()\n","                if grad.is_sparse:\n","                    raise RuntimeError('RAdam does not support sparse gradients')\n","\n","                p_data_fp32 = p.data.float()\n","\n","                state = self.state[p]\n","\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n","                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n","                else:\n","                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n","                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n","\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","                beta1, beta2 = group['betas']\n","\n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","\n","                state['step'] += 1\n","                beta2_t = beta2 ** state['step']\n","                N_sma_max = 2 / (1 - beta2) - 1\n","                N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n","\n","                if group['weight_decay'] != 0:\n","                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n","\n","                # more conservative since it's an approximated value\n","                if N_sma >= 5:\n","                    step_size = group['lr'] * math.sqrt(\n","                        (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (\n","                                    N_sma_max - 2)) / (1 - beta1 ** state['step'])\n","                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n","                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n","                else:\n","                    step_size = group['lr'] / (1 - beta1 ** state['step'])\n","                    p_data_fp32.add_(-step_size, exp_avg)\n","\n","                p.data.copy_(p_data_fp32)\n","\n","        return loss\n","\n","\n","class AdamW(Optimizer):\n","\n","    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, warmup=0):\n","        defaults = dict(lr=lr, betas=betas, eps=eps,\n","                        weight_decay=weight_decay, warmup=warmup)\n","        super(AdamW, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(AdamW, self).__setstate__(state)\n","\n","    def step(self, closure=None):\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","                grad = p.grad.data.float()\n","                if grad.is_sparse:\n","                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n","\n","                p_data_fp32 = p.data.float()\n","\n","                state = self.state[p]\n","\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n","                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n","                else:\n","                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n","                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n","\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","                beta1, beta2 = group['betas']\n","\n","                state['step'] += 1\n","\n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","\n","                denom = exp_avg_sq.sqrt().add_(group['eps'])\n","                bias_correction1 = 1 - beta1 ** state['step']\n","                bias_correction2 = 1 - beta2 ** state['step']\n","\n","                if group['warmup'] > state['step']:\n","                    scheduled_lr = 1e-8 + state['step'] * group['lr'] / group['warmup']\n","                else:\n","                    scheduled_lr = group['lr']\n","\n","                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n","\n","                if group['weight_decay'] != 0:\n","                    p_data_fp32.add_(-group['weight_decay'] * scheduled_lr, p_data_fp32)\n","\n","                p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n","\n","                p.data.copy_(p_data_fp32)\n","\n","        return loss"],"metadata":{"id":"JxGyPV68sD31","executionInfo":{"status":"ok","timestamp":1648302010410,"user_tz":-60,"elapsed":392,"user":{"displayName":"Rodrigo Castellano","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09621897243893675794"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"LJ-t_ep6dIF4"},"execution_count":null,"outputs":[]}]}